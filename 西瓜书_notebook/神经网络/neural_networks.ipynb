{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经元模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机与多层网络\n",
    ">感知机由两层神经元组成。输入层接收外界输入信号后传递给输出层，输出层是 M-P 神经元，亦称位\"阈值逻辑单元(threshold logic uint)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机能够轻易的实现逻辑与、或、非运算。\n",
    "\n",
    "$exmple$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阶跃函数\n",
    "def unit_step(x):\n",
    "    y = x>=0\n",
    "    return y.astype(np.int)\n",
    "\n",
    "# 逻辑运算\n",
    "def perc_logic(x,w,theta):\n",
    "    y = x@w.T-theta\n",
    "    out = unit_step(y)\n",
    "    return out\n",
    "x = np.array([[1,1],[0,1],[0,0],[1,0]])\n",
    "# 逻辑与\n",
    "w = np.array([1,1])\n",
    "theta = 2\n",
    "y = perc_logic(x,w,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [1 1 0 0]]\n",
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(x.T)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的权重是我们自己给的。一般的，给定训练数据集，权重 $w_i$ 以及阈值 $\\theta$ 可通过学习得到。感知机权重会根据如下方式进行调整：\n",
    "$$\n",
    "w_i\\gets w_i+\\Delta w_i\\\\\n",
    "\\Delta w_i=\\eta (y-\\hat y)x_i\n",
    "$$\n",
    "其中，$\\eta \\in (0,1)$ 称为学习率(learning rate)。感知机只能处理一些线性可分问题，其学习能力十分有限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.264  0.272 -0.442]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 感知机实现 逻辑与的学习\n",
    "import numpy as np\n",
    "# 阶跃函数\n",
    "def unit_step(x):\n",
    "    y = x>=0\n",
    "    return y.astype(np.int)\n",
    "\n",
    "# 创造数据集\n",
    "def setdata(n):\n",
    "    x1 = np.random.randint(0,2,n)\n",
    "    x2 = np.random.randint(0,2,n)\n",
    "    y = np.logical_and(x1,x2)\n",
    "    data = [x1,x2,y]\n",
    "    return np.array(data).T\n",
    "\n",
    "# 感知机\n",
    "class prec:\n",
    "\n",
    "    def __init__(self,w,lr,expoch):\n",
    "        self.w = w\n",
    "        self.lr = lr\n",
    "        self.expoch = expoch\n",
    "\n",
    "    def forward(self,x):\n",
    "        y1 = x@self.w.T\n",
    "        h1 = unit_step(y1)\n",
    "        out = h1\n",
    "        return out\n",
    "    \n",
    "    # 梯度更新\n",
    "    def gradient_fit(self,x,y):\n",
    "        out = self.forward(x)\n",
    "        gradient_w = np.sum(self.lr*(y-out)*x,axis=0)\n",
    "        self.w = self.w+gradient_w.T\n",
    "\n",
    "    def run(self,x,y):\n",
    "        for step in range(self.expoch):\n",
    "            self.gradient_fit(x,y)\n",
    "\n",
    "\n",
    "n = 1200\n",
    "rate = 0.8\n",
    "offset = int(n*rate)\n",
    "\n",
    "data = setdata(n)\n",
    "b = np.ones((n,1))\n",
    "data =  np.hstack((data,b))\n",
    "\n",
    "x_train,y_train,x_test,y_test = data[:offset,[0,1,-1]],data[:offset,2],data[offset:,[0,1,-1]],data[offset:,2]\n",
    "y_train = y_train.reshape((offset,1))\n",
    "y_test = y_test.reshape((n-offset,1))\n",
    "\n",
    "init_w = np.zeros((1,3))\n",
    "lr = 0.001\n",
    "expoch = 1000\n",
    "\n",
    "net = prec(init_w,lr,expoch)\n",
    "net.run(x_train,y_train)\n",
    "\n",
    "pre = unit_step(x_test@net.w.T)\n",
    "acc = 1-np.sum(np.abs(y_test-pre))/len(y_test)\n",
    "\n",
    "print(net.w)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 误差逆传播(BP)算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设隐含层和输出层神经元都使用 $Sigmoid$ 函数，则对训练例 $(x_k,y_k),y_k=(y_1^k,y_2^k,...,y_l^k)$ 有：\n",
    "\n",
    "$$\n",
    "\\hat y_j^k = f(\\beta_j - \\theta_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在网络上 $(x_k,y_k)$ 处的均方误差为：\n",
    "$$\n",
    "E_k = \\frac{1}{2}\\sum_{j=1}^{l}(\\hat y_j^k - y_j^k)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面网络中需要学习的参数有：$d\\times q + q\\times l + q+l$ 个。$BP$ 是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，任意参数 $v$ 的更新估计式为：\n",
    "$$\n",
    "v \\gets v + \\Delta v\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$BP$ 算法在基于梯度下降(gradient descent) 策略更新时，以目标的负梯度方向对参数进行调整。例如，对于上述的 $E_k$ ,给定学习率 $\\eta$ 时，有：\n",
    "$$\n",
    "\\Delta w_{hj}=-\\eta \\frac{\\partial E_k}{\\partial w_{hj}}\\\\\n",
    "\\frac{\\partial E_k}{\\partial w_{hj}}=\\frac{\\partial E_k}{\\partial \\hat y_{j}^k}\n",
    "·\\frac{\\partial \\hat y_{j}^k}{\\partial \\beta_j}·\\frac{\\partial \\beta_j}{\\partial w_{hj}}\\\\\n",
    "\\frac{\\partial \\beta_j}{\\partial w_{hj}}=b_h\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该网络中的 $Sigmoid$ 函数具如下良好的性质：\n",
    "$$\n",
    "f'(x)=f(x)·f(1-f(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是：\n",
    "$$\n",
    "g_i = -\\frac{\\partial E_k}{\\partial \\hat y_{j}^k}\n",
    "·\\frac{\\partial \\hat y_{j}^k}{\\partial \\beta_j}\\\\\n",
    "=-(\\hat y_j^k-y_j^k)f'(\\beta_j-\\theta_j)\\\\\n",
    "=\\hat y_j^k(1-\\hat y_j^k)(y_j^k-\\hat y_j^k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后得到 $BP$ 算法中关于 $w_{hj}$ 的更新公式：\n",
    "$$\n",
    "\\Delta w_{hj}=\\eta g_ib_h\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似的：\n",
    "$$\n",
    "\\Delta \\theta_j = -\\eta g_i\\\\\n",
    "\\Delta v_{ih} = \\eta e_hx_i\\\\\n",
    "\\Delta \\gamma_h=-\\eta e_h\\\\\n",
    "$$\n",
    "其中：\n",
    "$$\n",
    "e_h = -\\frac{\\partial E_k}{\\partial b_h}·\\frac{\\partial b_h}{\\partial \\alpha_h}\\\\\n",
    "=b_h(1-b_h)\\sum_{j=1}^{l}w_{hj}g_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，$BP$ 算法的目标是要最小化训练集 $D$ 上的累计误差：\n",
    "$$\n",
    "E = \\frac{1}{m}\\sum_{k=1}^{m}E_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "缓解过拟合策略：\n",
    "\n",
    "- 早停\n",
    ">将数据集分成训练集和验证集，训练集用来计算梯度，更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。\n",
    "\n",
    "- 正则化\n",
    ">在误差目标函数中增加一个用于描述网络复杂度的部分，例如：$E=\\lambda \\frac{1}{m}\\sum_{k=1}^{m}E_k+(1-\\lambda)\\sum_{i}w_i^2$，$\\lambda \\in (0,1)$ 用于对经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 避免局部最小\n",
    "\n",
    "- 以多组不同参数值初始化多个神经网络\n",
    "- 使用“模拟退火”\n",
    "- 使用随机梯度下降\n",
    "- 遗传算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他常见神经网络\n",
    "\n",
    "- $RBF$ 网络\n",
    "\n",
    "- $ART$ 网络\n",
    "\n",
    "- $SOM$ 网络\n",
    "\n",
    "- 级联相关网络\n",
    "\n",
    "- $Elman$ 网络\n",
    "\n",
    "- $Boltzmann$ 机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个单隐层 $BP$ 算法的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# 对中文的支持\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y\n",
    "\n",
    "class SimpleNet:\n",
    "\n",
    "    def __init__(self,w,b,lr,expoch):\n",
    "\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.lr = lr\n",
    "        self.expoch = expoch\n",
    "\n",
    "    def forwad(self,x):\n",
    "\n",
    "        y1 = self.w[0].T@x+self.b[0]\n",
    "        h1 = sigmoid(y1)\n",
    "        y2 = self.w[1].T@h1+self.b[1]\n",
    "        h2 = sigmoid(y2)\n",
    "        out = h2\n",
    "        return out,h1\n",
    "\n",
    "    def loss(self,y,ylabel):\n",
    "\n",
    "        Ek = np.sum((y-ylabel)**2)/2\n",
    "\n",
    "        return Ek\n",
    "\n",
    "    def gradient_gj_eh(self,x,y,ylabel,h1):\n",
    "\n",
    "        gj = y*(1-y)*(ylabel-y)\n",
    "\n",
    "        eh = h1*(1-h1)*(self.w[1])@gj\n",
    "        grad_w1 = self.lr*x@eh.T\n",
    "        grad_w2 = self.lr*h1@gj.T\n",
    "\n",
    "        grad_b1 = -self.lr*eh\n",
    "        grad_b2 = -self.lr*gj\n",
    "\n",
    "        self.w[0] += grad_w1\n",
    "        self.w[1] += grad_w2\n",
    "\n",
    "        self.b[0] += grad_b1\n",
    "        self.b[1] += grad_b2\n",
    "\n",
    "\n",
    "    def fit(self,x_train,y_train):\n",
    "        m = x_train.shape[-1]\n",
    "\n",
    "        for step in range(self.expoch):\n",
    "            ls = 0\n",
    "            for k in range(m):\n",
    "                x = np.array(x_train[:,k],ndmin=2).T\n",
    "                ylabel = np.array(y_train[:, k], ndmin=2).T\n",
    "                out,h1 = self.forwad(x)\n",
    "                ls += self.loss(out,ylabel)\n",
    "\n",
    "                self.gradient_gj_eh(x,out,ylabel,h1)\n",
    "\n",
    "            E = ls / m\n",
    "            if step%50==0:\n",
    "                print(E,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = 10 # 输入层样本数量\n",
    "xd = 2 # 输入层样本属性（输入层神经元数量）\n",
    "hq = 5 # 隐含层神经元数量\n",
    "outl = 1 # 输出层神经元数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones((xd,xm))\n",
    "\n",
    "y = np.ones((outl,xm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.zeros((xd,hq))\n",
    "w2 = np.zeros((hq,outl))\n",
    "w = [w1,w2]\n",
    "\n",
    "b1 = np.zeros((hq,1))\n",
    "b2 = np.zeros((outl,1))\n",
    "b = [b1,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNet(w,b,lr=0.01,expoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1248243769260496 [[0.5007027]]\n",
      "0.10554248349200798 [[0.54095249]]\n"
     ]
    }
   ],
   "source": [
    "net.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}